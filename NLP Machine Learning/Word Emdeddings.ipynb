{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73496c7c",
   "metadata": {},
   "source": [
    "##### What is word embeddings and why do we need them?\n",
    "\n",
    "__Word embeddings__ - Words that appear in similar contexts tend to have similar meanings. This idea comes from linguistics and is called _The Distributional Hypothesis_.\n",
    "\n",
    "__Core Limitation of Classical Methods (N-grams, BOW, TF IDF)__:\n",
    "- treat words as independent symbols\n",
    "\n",
    "- rely on counts\n",
    "\n",
    "- live in high-dimensional sparse space\n",
    "\n",
    "- have no notion of meaning\n",
    "\n",
    "So for classical NLP: fraud ‚â† scam ‚â† cheating\n",
    "\n",
    "They are:\n",
    "\n",
    "- different columns\n",
    "\n",
    "- unrelated vectors\n",
    "\n",
    "- orthogonal dimensions\n",
    "\n",
    "But humans know they are related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a9a2b",
   "metadata": {},
   "source": [
    "##### Intution:\n",
    "\n",
    "Examples:\n",
    "- He deposited money _in_ the bank.\n",
    "- She withdrew cash _from_ the bank.\n",
    "\n",
    "vs\n",
    "- He sat _by_ the river bank.\n",
    "- The bank was flooded\n",
    "\n",
    "The word bank appears in different contexts ‚Üí different meanings.\n",
    "Embeddings try to encode context information into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b97fb",
   "metadata": {},
   "source": [
    "##### So a word embededding, conceptually, is:\n",
    "- a desnse vector\n",
    "- low dimensional (upto 300 dimensions)\n",
    "- learned from data\n",
    "- where distance and direction carry meaning.\n",
    "\n",
    "So, instead of this (BoW):\n",
    "\n",
    "        [0, 0, 1, 0, 0, 0, 0, ...]  # sparse\n",
    "\n",
    "\n",
    "We get this:\n",
    "\n",
    "        [0.12, -0.44, 0.87, ...]   # dense\n",
    "\n",
    "\n",
    "Each number has no standalone meaning ‚Äî meaning comes from geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad349f",
   "metadata": {},
   "source": [
    "__Important note:__ Embeddings do not store meaning. They store usage patterns, meanings emerges from usage.\n",
    "\n",
    "So embedding:\n",
    "- captures bias\n",
    "- reflect data distribution\n",
    "- can drift over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5bd95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281f67b",
   "metadata": {},
   "source": [
    "##### Pre - requisites before Word2Vec:\n",
    "\n",
    "__Artificial Neural Network (ANN)__\n",
    "\n",
    "An ANN is a function approximator inspired by biological neurons.\n",
    "\n",
    "\n",
    "Basic structure\n",
    "\n",
    "- Input layer: receives features\n",
    "\n",
    "- Hidden layers: apply weighted sums + nonlinear activations\n",
    "\n",
    "- Output layer: produces predictions\n",
    "\n",
    "The neuron computes $y = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$. where ùëì is an activation function (e.g., ReLU, sigmoid).\n",
    "\n",
    "_Purpose:_ learn a mapping from inputs to outputs by adjusting weights.\n",
    "\n",
    "Think of an ANN as a machine that learns from examples.\n",
    "\n",
    "- You give it input (data)\n",
    "\n",
    "- It makes a guess\n",
    "\n",
    "- It slowly learns how to make better guesses\n",
    "\n",
    "Inside, it has many small units (‚Äúneurons‚Äù) that:\n",
    "\n",
    "- Look at the input\n",
    "\n",
    "- Decide what is important\n",
    "\n",
    "- Pass the result forward\n",
    "\n",
    "Over time, the network learns which inputs matter more than others.\n",
    "\n",
    "__Loss Function__\n",
    "\n",
    "The loss function measures how wrong the model‚Äôs predictions are.\n",
    "\n",
    "- Scalar value\n",
    "\n",
    "- Lower loss = better performance\n",
    "\n",
    "- Guides learning\n",
    "\n",
    "Examples\n",
    "\n",
    "- _Mean Squared Error (MSE) ‚Äî regression_\n",
    "- _Cross-Entropy Loss ‚Äî classification : Penalizes confident wrong predictions_\n",
    "\n",
    "The loss function is simply a score of how wrong the guess was.\n",
    "\n",
    "- If the guess is good ‚Üí low loss\n",
    "\n",
    "- If the guess is bad ‚Üí high loss\n",
    "\n",
    "Example:\n",
    "\n",
    "- True answer: 10\n",
    "\n",
    "- Model guesses: 50 ‚Üí big loss\n",
    "\n",
    "- Model guesses: 11 ‚Üí small loss\n",
    "\n",
    "The model uses this score to know how much it needs to improve.\n",
    "\n",
    "\n",
    "__Optimizer__\n",
    "\n",
    "The optimizer defines how weights are updated to minimize the loss.\n",
    "\n",
    "Core idea:\n",
    "\n",
    "ùë§\n",
    "‚Üê\n",
    "ùë§\n",
    "‚àí\n",
    "ùúÇ\n",
    "‚àá\n",
    "ùêø\n",
    "\n",
    "\n",
    "where \n",
    "ùúÇ is the learning rate.\n",
    "\n",
    "Common optimizers\n",
    "\n",
    "- SGD: simple, stable, slower convergence\n",
    "\n",
    "- Momentum: accelerates SGD using past gradients\n",
    "\n",
    "- Adam: adaptive learning rates; fast and widely used\n",
    "\n",
    "Optimizer choice affects:\n",
    "\n",
    "- Training speed\n",
    "\n",
    "- Stability\n",
    "\n",
    "- Final model quality\n",
    "\n",
    "The optimizer is the method the model uses to improve itself.\n",
    "\n",
    "After seeing how wrong it was:\n",
    "\n",
    "- The optimizer changes the model slightly\n",
    "\n",
    "- The goal is to reduce future mistakes\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "- Loss function: ‚ÄúHow bad was I?‚Äù\n",
    "\n",
    "- Optimizer: ‚ÄúHow should I adjust next time?‚Äù\n",
    "\n",
    "Different optimizers adjust more cautiously or more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426528eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c81c9b",
   "metadata": {},
   "source": [
    "##### Word2Vec\n",
    "\n",
    "It is a NLP technique that uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can:\n",
    "- detect synonymous words\n",
    "- suggest additonal words for a partial sentence.\n",
    "\n",
    "As the name implies, it represents each distinct word with a particular list of numbers called a vector.\n",
    "\n",
    "__It is a method to learn word embeddings from text.__\n",
    "\n",
    "- Word2Vec = a way to train word vectors such that words with similar usage patterns have similar vectors.\n",
    "\n",
    "The output is:\n",
    "\n",
    " - a dense vector per word\n",
    "\n",
    " - typically 50‚Äì300 dimensions\n",
    "\n",
    " - learned automatically from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4f462",
   "metadata": {},
   "source": [
    "##### The Problem Word2Vec Solves:\n",
    "\n",
    " - Classical NLP sees this:\n",
    "\n",
    "        - fraud ‚â† scam ‚â† cheating\n",
    "\n",
    "\n",
    "- Word2Vec tries to make this true in vector space:\n",
    "\n",
    "        - cosine_similarity(fraud, scam) ‚Üí high\n",
    "        - cosine_similarity(fraud, banana) ‚Üí low\n",
    "\n",
    "\n",
    "It does this by learning from context, not frequency.\n",
    "\n",
    "\n",
    "__Important :__ A word is defined by the words that surround it. This is the distributional hypothesis in action.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"free prize win now\"\n",
    "\n",
    "\n",
    "The word prize often appears near:\n",
    "\n",
    "- free\n",
    "- win\n",
    "- claim\n",
    "- money\n",
    "\n",
    "So its vector should be close to those words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee68d3",
   "metadata": {},
   "source": [
    "##### Cosine Similarity\n",
    "\n",
    "n NLP, words and documents become vectors.\n",
    "\n",
    "We need a way to answer:\n",
    "- ‚ÄúHow similar are these two vectors?‚Äù\n",
    "\n",
    "But similarity should mean:\n",
    "\n",
    "- meaning similarity, not size similarity\n",
    "- independent of how long a document is\n",
    "\n",
    "This is why cosine similarity exists. It calculates how aligned two vectors are in direction.\n",
    "- same direction ‚Üí similar meaning\n",
    "- opposite direction ‚Üí opposite meaning\n",
    "- perpendicular ‚Üí unrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49432690",
   "metadata": {},
   "source": [
    "__Cosine Similarity Formula:__ It is the angle between the two vectors.\n",
    "\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\, ||\\vec{B}||}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97840d6",
   "metadata": {},
   "source": [
    "\n",
    "        ¬∑ = dot product\n",
    "\n",
    "        ||A|| = magnitude (length) of vector A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25baceb3",
   "metadata": {},
   "source": [
    "A = [1, 0, 1]\n",
    "B = [1, 0, 1]\n",
    "\n",
    "$$\n",
    "\\vec{A} \\cdot \\vec{B} = (a_1 \\times b_1) + (a_2 \\times b_2) + (a_3 \\times b_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{A} \\cdot \\vec{B} = (1 \\times 1) + (0 \\times 0) + (1 \\times 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0 + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0 + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| = \\sqrt{a_1^2 + a_2^2 + a_3^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| = \\sqrt{1^2 + 0^2 + 1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{1 + 0 + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{B}|| = \\sqrt{1^2 + 0^2 + 1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| \\times ||\\vec{B}|| = \\sqrt{2} \\times \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{2}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\cos(\\theta) = 1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648a927",
   "metadata": {},
   "source": [
    "__Distance between the vectors__:\n",
    "\n",
    "$$\n",
    "\\boxed {1 - \\cos(\\theta)}\n",
    "$$\n",
    "\n",
    "_So the distance is 0. This means the vecotrs are similar._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb60c0",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words) : A subset of Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8669e",
   "metadata": {},
   "source": [
    "CBOW predicts a target word given its surrounding context words.\n",
    "\n",
    "Example\n",
    "\n",
    "Sentence:\n",
    "\n",
    "        the cat sat on the mat\n",
    "\n",
    "\n",
    "With window size = 2 and target = sat:\n",
    "\n",
    "- Input (context words): [\"the\", \"cat\", \"on\", \"the\"]\n",
    "\n",
    "- Output (target word): \"sat\"\n",
    "\n",
    "Order of context words does not matter ‚Üí ‚Äúbag of words‚Äù.\n",
    "\n",
    "__Important__:\n",
    "- __Window size__ plays a crucial role in identifying the input and output data. Its good practice to take the window size as an _odd_ number.\n",
    "    - It helps us to identify number the words we need to pick in the beginiing. Out of these 5 words, we will take the center word.\n",
    "    - This will tell us the context of the forward and the backward words. \n",
    "    - __NOTE__: _Window size only decides which words are used as input. It does not define the neural network size_\n",
    "    \n",
    "    For example in _Mayank is on his way to learn NLP_ if I select the window size to be 5 from the beginning :\n",
    "\n",
    "        - Input1 = [Mayank, is, his, way]\n",
    "        - Output1 = [on]\n",
    "    - Moving the window to the next 5 words in the same example\n",
    "        - Input2 = [is,on,way,to]\n",
    "        - Output2 = [his]\n",
    "    - Moving again\n",
    "        - Input3 =[on,his,to,learn]\n",
    "        - Output3 = [way]\n",
    "    - Moving again\n",
    "        - Input4 = [his, way, learn, NLP]\n",
    "        - Output4 = [to]    \n",
    "    \n",
    "    Now we will train our model with this data. In order to achieve that, we need to conver the data into vectors. We can use methods like OneHot Encoding.\n",
    "\n",
    "    - Let's perform OHE\n",
    "\n",
    "    Vocabulary = [Mayank, is, on, his, way, to, learn, NLP]\n",
    "\n",
    "    OHE:\n",
    "    - Mayank [1,0,0,0,0,0,0,0]\n",
    "    - is [0,1,0,0,0,0,0,0]\n",
    "    - on [0,0,1,0,0,0,0,0]\n",
    "    - his [0,0,0,1,0,0,0,0]\n",
    "    - way [0,0,0,0,1,0,0,0]\n",
    "    - to [0,0,0,0,0,1,0,0]\n",
    "    - learn [0,0,0,0,0,0,1,0]\n",
    "    - NLP [0,0,0,0,0,0,0,1]\n",
    "\n",
    "    So, each word is represented as an 8-dimensional one-hot vector.\n",
    "\n",
    "    Let's train the model now. \n",
    "    \n",
    "    __Let's choose a target word.__\n",
    "    - 'his' is the target word.\n",
    "    - since window size is 5, left : _is,on_ , right : _way,to_\n",
    "\n",
    "    __Input Layer__\n",
    "    The input is multiple one-hot vectors, one for each context words. \n",
    "    - In our case, context words are: _[is,on,way,to]_\n",
    "    - Their one hot vectors:\n",
    "        - is [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        - on [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "        - way [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "        - to [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "    __Hidden Layer (Embedding Layer)__\n",
    "\n",
    "    This is a shared respresentation for all the words and exists once for the entire model. More dimensions = more capacity to store semantic information.\n",
    "\n",
    "    Between input and hidden layer:\n",
    "    - Weight matrix size = vocab size * embedding dimension\n",
    "    - For our example, let us take embedding dimension as 4.\n",
    "    - So __W = 8 x 4__\n",
    "\n",
    "    __So what is an embedding dimension?__\n",
    "    The number of numbers used to represent the meaning of a word.\n",
    "\n",
    "            With OHE, is = [0, 1, 0, 0, 0, 0, 0, 0].\n",
    "            With embedding dimension 4, is = [0.12,.0,15,0.66,0.88]\n",
    "\n",
    "    This means each word gets mapped to a 4D dense vector.\n",
    "\n",
    "    How context words are combined:\n",
    "\n",
    "    - Each one-hot vector selects its row from W\n",
    "\n",
    "    - This gives 4 embedding vectors\n",
    "\n",
    "    - CBOW averages them\n",
    "\n",
    "    Mathematically (conceptually):\n",
    "\n",
    "        hidden_vector =    (embedding(is) + embedding(on) + embedding(way) + embedding(to)) / 4\n",
    "    \n",
    "    _This averaged vector is the hidden layer output._\n",
    "\n",
    "\n",
    "    __Output Layer__\n",
    "    The output layer tries to predict the target word (‚Äúhis‚Äù). \n",
    "    - It tries to answer - Given this context representation, which word best fits in the center?\n",
    "    - It must assign a score or probability to every word in the vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07175396",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
