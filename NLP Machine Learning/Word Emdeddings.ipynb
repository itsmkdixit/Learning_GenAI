{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73496c7c",
   "metadata": {},
   "source": [
    "##### What is word embeddings and why do we need them?\n",
    "\n",
    "__Word embeddings__ - Words that appear in similar contexts tend to have similar meanings. This idea comes from linguistics and is called _The Distributional Hypothesis_.\n",
    "\n",
    "__Core Limitation of Classical Methods (N-grams, BOW, TF IDF)__:\n",
    "- treat words as independent symbols\n",
    "\n",
    "- rely on counts\n",
    "\n",
    "- live in high-dimensional sparse space\n",
    "\n",
    "- have no notion of meaning\n",
    "\n",
    "So for classical NLP: fraud ≠ scam ≠ cheating\n",
    "\n",
    "They are:\n",
    "\n",
    "- different columns\n",
    "\n",
    "- unrelated vectors\n",
    "\n",
    "- orthogonal dimensions\n",
    "\n",
    "But humans know they are related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a9a2b",
   "metadata": {},
   "source": [
    "##### Intution:\n",
    "\n",
    "Examples:\n",
    "- He deposited money _in_ the bank.\n",
    "- She withdrew cash _from_ the bank.\n",
    "\n",
    "vs\n",
    "- He sat _by_ the river bank.\n",
    "- The bank was flooded\n",
    "\n",
    "The word bank appears in different contexts → different meanings.\n",
    "Embeddings try to encode context information into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b97fb",
   "metadata": {},
   "source": [
    "##### So a word embededding, conceptually, is:\n",
    "- a desnse vector\n",
    "- low dimensional (upto 300 dimensions)\n",
    "- learned from data\n",
    "- where distance and direction carry meaning.\n",
    "\n",
    "So, instead of this (BoW):\n",
    "\n",
    "        [0, 0, 1, 0, 0, 0, 0, ...]  # sparse\n",
    "\n",
    "\n",
    "We get this:\n",
    "\n",
    "        [0.12, -0.44, 0.87, ...]   # dense\n",
    "\n",
    "\n",
    "Each number has no standalone meaning — meaning comes from geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad349f",
   "metadata": {},
   "source": [
    "__Important note:__ Embeddings do not store meaning. They store usage patterns, meanings emerges from usage.\n",
    "\n",
    "So embedding:\n",
    "- captures bias\n",
    "- reflect data distribution\n",
    "- can drift over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c81c9b",
   "metadata": {},
   "source": [
    "##### Word2Vec\n",
    "\n",
    "It is a NLP technique that uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can:\n",
    "- detect synonymous words\n",
    "- suggest additonal words for a partial sentence.\n",
    "\n",
    "As the name implies, it represents each distinct word with a particular list of numbers called a vector.\n",
    "\n",
    "__It is a method to learn word embeddings from text.__\n",
    "\n",
    "- Word2Vec = a way to train word vectors such that words with similar usage patterns have similar vectors.\n",
    "\n",
    "The output is:\n",
    "\n",
    " - a dense vector per word\n",
    "\n",
    " - typically 50–300 dimensions\n",
    "\n",
    " - learned automatically from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4f462",
   "metadata": {},
   "source": [
    "##### The Problem Word2Vec Solves:\n",
    "\n",
    " - Classical NLP sees this:\n",
    "\n",
    "        - fraud ≠ scam ≠ cheating\n",
    "\n",
    "\n",
    "- Word2Vec tries to make this true in vector space:\n",
    "\n",
    "        - cosine_similarity(fraud, scam) → high\n",
    "        - cosine_similarity(fraud, banana) → low\n",
    "\n",
    "\n",
    "It does this by learning from context, not frequency.\n",
    "\n",
    "\n",
    "__Important :__ A word is defined by the words that surround it. This is the distributional hypothesis in action.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"free prize win now\"\n",
    "\n",
    "\n",
    "The word prize often appears near:\n",
    "\n",
    "- free\n",
    "- win\n",
    "- claim\n",
    "- money\n",
    "\n",
    "So its vector should be close to those words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee68d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
