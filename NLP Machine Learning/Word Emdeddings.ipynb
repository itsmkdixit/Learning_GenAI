{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73496c7c",
   "metadata": {},
   "source": [
    "##### What is word embeddings and why do we need them?\n",
    "\n",
    "__Word embeddings__ - Words that appear in similar contexts tend to have similar meanings. This idea comes from linguistics and is called _The Distributional Hypothesis_.\n",
    "\n",
    "__Core Limitation of Classical Methods (N-grams, BOW, TF IDF)__:\n",
    "- treat words as independent symbols\n",
    "\n",
    "- rely on counts\n",
    "\n",
    "- live in high-dimensional sparse space\n",
    "\n",
    "- have no notion of meaning\n",
    "\n",
    "So for classical NLP: fraud ≠ scam ≠ cheating\n",
    "\n",
    "They are:\n",
    "\n",
    "- different columns\n",
    "\n",
    "- unrelated vectors\n",
    "\n",
    "- orthogonal dimensions\n",
    "\n",
    "But humans know they are related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a9a2b",
   "metadata": {},
   "source": [
    "##### Intution:\n",
    "\n",
    "Examples:\n",
    "- He deposited money _in_ the bank.\n",
    "- She withdrew cash _from_ the bank.\n",
    "\n",
    "vs\n",
    "- He sat _by_ the river bank.\n",
    "- The bank was flooded\n",
    "\n",
    "The word bank appears in different contexts → different meanings.\n",
    "Embeddings try to encode context information into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b97fb",
   "metadata": {},
   "source": [
    "##### So a word embededding, conceptually, is:\n",
    "- a desnse vector\n",
    "- low dimensional (upto 300 dimensions)\n",
    "- learned from data\n",
    "- where distance and direction carry meaning.\n",
    "\n",
    "So, instead of this (BoW):\n",
    "\n",
    "        [0, 0, 1, 0, 0, 0, 0, ...]  # sparse\n",
    "\n",
    "\n",
    "We get this:\n",
    "\n",
    "        [0.12, -0.44, 0.87, ...]   # dense\n",
    "\n",
    "\n",
    "Each number has no standalone meaning — meaning comes from geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad349f",
   "metadata": {},
   "source": [
    "__Important note:__ Embeddings do not store meaning. They store usage patterns, meanings emerges from usage.\n",
    "\n",
    "So embedding:\n",
    "- captures bias\n",
    "- reflect data distribution\n",
    "- can drift over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c81c9b",
   "metadata": {},
   "source": [
    "##### Word2Vec\n",
    "\n",
    "It is a NLP technique that uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can:\n",
    "- detect synonymous words\n",
    "- suggest additonal words for a partial sentence.\n",
    "\n",
    "As the name implies, it represents each distinct word with a particular list of numbers called a vector.\n",
    "\n",
    "__It is a method to learn word embeddings from text.__\n",
    "\n",
    "- Word2Vec = a way to train word vectors such that words with similar usage patterns have similar vectors.\n",
    "\n",
    "The output is:\n",
    "\n",
    " - a dense vector per word\n",
    "\n",
    " - typically 50–300 dimensions\n",
    "\n",
    " - learned automatically from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4f462",
   "metadata": {},
   "source": [
    "##### The Problem Word2Vec Solves:\n",
    "\n",
    " - Classical NLP sees this:\n",
    "\n",
    "        - fraud ≠ scam ≠ cheating\n",
    "\n",
    "\n",
    "- Word2Vec tries to make this true in vector space:\n",
    "\n",
    "        - cosine_similarity(fraud, scam) → high\n",
    "        - cosine_similarity(fraud, banana) → low\n",
    "\n",
    "\n",
    "It does this by learning from context, not frequency.\n",
    "\n",
    "\n",
    "__Important :__ A word is defined by the words that surround it. This is the distributional hypothesis in action.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"free prize win now\"\n",
    "\n",
    "\n",
    "The word prize often appears near:\n",
    "\n",
    "- free\n",
    "- win\n",
    "- claim\n",
    "- money\n",
    "\n",
    "So its vector should be close to those words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee68d3",
   "metadata": {},
   "source": [
    "##### Cosine Similarity\n",
    "\n",
    "n NLP, words and documents become vectors.\n",
    "\n",
    "We need a way to answer:\n",
    "- “How similar are these two vectors?”\n",
    "\n",
    "But similarity should mean:\n",
    "\n",
    "- meaning similarity, not size similarity\n",
    "- independent of how long a document is\n",
    "\n",
    "This is why cosine similarity exists. It calculates how aligned two vectors are in direction.\n",
    "- same direction → similar meaning\n",
    "- opposite direction → opposite meaning\n",
    "- perpendicular → unrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49432690",
   "metadata": {},
   "source": [
    "__Cosine Similarity Formula:__ It is the angle between the two vectors.\n",
    "\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\, ||\\vec{B}||}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97840d6",
   "metadata": {},
   "source": [
    "\n",
    "        · = dot product\n",
    "\n",
    "        ||A|| = magnitude (length) of vector A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25baceb3",
   "metadata": {},
   "source": [
    "A = [1, 0, 1]\n",
    "B = [1, 0, 1]\n",
    "\n",
    "$$\n",
    "\\vec{A} \\cdot \\vec{B} = (a_1 \\times b_1) + (a_2 \\times b_2) + (a_3 \\times b_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{A} \\cdot \\vec{B} = (1 \\times 1) + (0 \\times 0) + (1 \\times 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0 + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0 + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| = \\sqrt{a_1^2 + a_2^2 + a_3^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| = \\sqrt{1^2 + 0^2 + 1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{1 + 0 + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{B}|| = \\sqrt{1^2 + 0^2 + 1^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "||\\vec{A}|| \\times ||\\vec{B}|| = \\sqrt{2} \\times \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{2}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\cos(\\theta) = 1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648a927",
   "metadata": {},
   "source": [
    "__Distance between the vectors__:\n",
    "\n",
    "$$\n",
    "\\boxed {1 - \\cos(\\theta)}\n",
    "$$\n",
    "\n",
    "_So the distance is 0. This means the vecotrs are similar._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb60c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d8669e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
